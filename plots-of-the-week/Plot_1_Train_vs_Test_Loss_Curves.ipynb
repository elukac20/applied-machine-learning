{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a0e9376e",
      "metadata": {
        "id": "a0e9376e"
      },
      "source": [
        "# Plot of the Week: Train vs Validation Loss Curves (Overfitting)\n",
        "\n",
        "**Goal.** Fit a **nonlinear regression** model and plot **train vs validation loss vs epoch** to diagnose **overfitting**.\n",
        "\n",
        "Students will **modify one element** (model size, regularization, optimizer hyperparameters, dataset size/noise, etc.) to **prevent overfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "## What question does this plot answer?\n",
        "How do the model’s training and validation losses evolve during training, and **does the gap indicate overfitting**?\n",
        "\n",
        "## Why is it important?\n",
        "Loss curves are a fast, reliable diagnostic for:\n",
        "- **Overfitting vs underfitting**\n",
        "- **Optimization stability** (learning rate too high/low, divergence, plateaus)\n",
        "- **Experiment tracking** (comparing runs and hyperparameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d443ffc5",
      "metadata": {
        "id": "d443ffc5"
      },
      "outputs": [],
      "source": [
        "# If you're running this on Colab, you might need:\n",
        "# !pip install torch --quiet\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90843245",
      "metadata": {
        "id": "90843245"
      },
      "source": [
        "## 1) Create a simple nonlinear regression dataset (simulated)\n",
        "\n",
        "We’ll learn a noisy nonlinear function (a sinusoid with noise).\n",
        "To make overfitting **easy to observe**, we use:\n",
        "- relatively **small training set**\n",
        "- a **high-capacity** model later\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52be9936",
      "metadata": {
        "id": "52be9936"
      },
      "outputs": [],
      "source": [
        "def make_sine_data(n_train=64, n_val=256, noise_std=0.25, x_range=(-3.0, 3.0)):\n",
        "    # Train points\n",
        "    x_train = np.random.uniform(x_range[0], x_range[1], size=(n_train, 1)).astype(np.float32)\n",
        "    y_train = (np.sin(2.5 * x_train) + 0.3*np.cos(6 * x_train) + noise_std*np.random.randn(n_train, 1)).astype(np.float32)\n",
        "\n",
        "    # Validation points (denser grid)\n",
        "    x_val = np.linspace(x_range[0], x_range[1], n_val).reshape(-1, 1).astype(np.float32)\n",
        "    y_val = (np.sin(2.5 * x_val) + 0.3*np.cos(6 * x_val) + noise_std*np.random.randn(n_val, 1)).astype(np.float32)\n",
        "    return x_train, y_train, x_val, y_val\n",
        "\n",
        "x_train, y_train, x_val, y_val = make_sine_data()\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x_train, y_train, s=18, label=\"train (noisy samples)\")\n",
        "plt.scatter(x_val, y_val, s=8, alpha=0.35, label=\"val (noisy samples)\")\n",
        "plt.title(\"Nonlinear regression dataset\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd710827",
      "metadata": {
        "id": "fd710827"
      },
      "source": [
        "## 2) Define a nonlinear regression model (MLP)\n",
        "\n",
        "This MLP can easily **overfit** if:\n",
        "- too many hidden units/layers\n",
        "- training too long\n",
        "- weak/no regularization\n",
        "\n",
        "**Student knobs** (change these later): `hidden_dim`, `depth`, `dropout`, `weight_decay`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86d4290f",
      "metadata": {
        "id": "86d4290f"
      },
      "outputs": [],
      "source": [
        "class MLPRegressor(nn.Module):\n",
        "    def __init__(self, in_dim=1, hidden_dim=256, depth=4, dropout=0.0):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        d = in_dim\n",
        "        for _ in range(depth):\n",
        "            layers.append(nn.Linear(d, hidden_dim))\n",
        "            layers.append(nn.Tanh())  # smooth nonlinearity works well for this toy regression\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            d = hidden_dim\n",
        "        layers.append(nn.Linear(d, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Baseline model (intentionally high-capacity to provoke overfitting)\n",
        "model = MLPRegressor(hidden_dim=256, depth=4, dropout=0.0).to(device)\n",
        "sum(p.numel() for p in model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da07ae6",
      "metadata": {
        "id": "7da07ae6"
      },
      "source": [
        "## 3) Training loop with logging\n",
        "\n",
        "We log:\n",
        "- training loss per epoch\n",
        "- validation loss per epoch\n",
        "\n",
        "and then plot them.\n",
        "\n",
        "**Student knobs**: `lr`, `weight_decay`, `batch_size`, `epochs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e57d972e",
      "metadata": {
        "id": "e57d972e"
      },
      "outputs": [],
      "source": [
        "def train_one_run(\n",
        "    model,\n",
        "    x_train, y_train, x_val, y_val,\n",
        "    lr=3e-3,\n",
        "    weight_decay=0.0,\n",
        "    batch_size=32,\n",
        "    epochs=300,\n",
        "    print_every=50,\n",
        "):\n",
        "    # Data\n",
        "    train_ds = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
        "    val_ds   = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    # Loss + optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        n = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total += loss.item() * xb.size(0)\n",
        "            n += xb.size(0)\n",
        "        train_loss = total / n\n",
        "\n",
        "        # ---- val ----\n",
        "        model.eval()\n",
        "        total = 0.0\n",
        "        n = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss = criterion(pred, yb)\n",
        "                total += loss.item() * xb.size(0)\n",
        "                n += xb.size(0)\n",
        "        val_loss = total / n\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if print_every and (epoch % print_every == 0 or epoch == 1 or epoch == epochs):\n",
        "            print(f\"epoch {epoch:4d} | train MSE {train_loss:.4f} | val MSE {val_loss:.4f}\")\n",
        "\n",
        "    return np.array(train_losses), np.array(val_losses)\n",
        "\n",
        "# Run a baseline that often overfits\n",
        "model = MLPRegressor(hidden_dim=256, depth=4, dropout=0.0).to(device)\n",
        "\n",
        "train_losses, val_losses = train_one_run(\n",
        "    model,\n",
        "    x_train, y_train, x_val, y_val,\n",
        "    lr=3e-3,\n",
        "    weight_decay=0.0,\n",
        "    batch_size=32,\n",
        "    epochs=300,\n",
        "    print_every=60,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b795ad37",
      "metadata": {
        "id": "b795ad37"
      },
      "source": [
        "## 4) Plot 1: Train vs Validation Loss Curves (minimum requirement)\n",
        "\n",
        "**Interpretation tip:**  \n",
        "Overfitting often looks like:\n",
        "- train loss keeps decreasing\n",
        "- validation loss bottoms out then starts increasing (or stops improving)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0caaddd",
      "metadata": {
        "id": "f0caaddd"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(train_losses, label=\"train loss\")\n",
        "plt.plot(val_losses, label=\"validation loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"MSE loss\")\n",
        "plt.title(\"Train vs Validation Loss Curves\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Optional: print epoch of best validation performance\n",
        "best_epoch = int(val_losses.argmin()) + 1\n",
        "print(\"Best validation loss at epoch:\", best_epoch, \"val_loss:\", float(val_losses.min()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeeb2c81",
      "metadata": {
        "id": "aeeb2c81"
      },
      "source": [
        "## 5) Visualize the fitted function (helps explain overfitting)\n",
        "\n",
        "This is optional but often makes the story clearer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "258c167a",
      "metadata": {
        "id": "258c167a"
      },
      "outputs": [],
      "source": [
        "# Predict on a dense grid\n",
        "model.eval()\n",
        "x_grid = np.linspace(-3, 3, 600).reshape(-1, 1).astype(np.float32)\n",
        "with torch.no_grad():\n",
        "    y_pred = model(torch.from_numpy(x_grid).to(device)).cpu().numpy()\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x_train, y_train, s=18, label=\"train\")\n",
        "plt.plot(x_grid, np.sin(2.5*x_grid) + 0.3*np.cos(6*x_grid), linewidth=2, label=\"true function (noise-free)\")\n",
        "plt.plot(x_grid, y_pred, linewidth=2, label=\"model prediction\")\n",
        "plt.title(\"Fit visualization (optional)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c2799b",
      "metadata": {
        "id": "41c2799b"
      },
      "source": [
        "## 6) Student task: prevent overfitting\n",
        "\n",
        "**Choose ONE change** and re-run the training cell(s). Then submit your updated **loss curve plot**.\n",
        "\n",
        "### Suggested edits (pick one)\n",
        "1. **Regularization**  \n",
        "   - increase `weight_decay` (e.g., `1e-4`, `1e-3`, `1e-2`)  \n",
        "   - add `dropout` (e.g., `0.1`–`0.3`)  \n",
        "2. **Reduce model capacity**  \n",
        "   - lower `hidden_dim` (e.g., `32`, `64`)  \n",
        "   - reduce `depth` (e.g., `2`)  \n",
        "3. **Train less / early stopping (simple version)**  \n",
        "   - reduce `epochs`  \n",
        "   - stop training near the best validation epoch  \n",
        "4. **More data / less noise**  \n",
        "   - increase `n_train` (e.g., `256`)  \n",
        "   - reduce `noise_std` (e.g., `0.10`)  \n",
        "5. **Optimizer dynamics**  \n",
        "   - change `lr` (try `1e-3` or `1e-4`)  \n",
        "   - change `batch_size`\n",
        "\n",
        "### What “success” looks like\n",
        "A run where **validation loss does not rise** (or rises much less), and the **train–val gap** is smaller.\n",
        "\n",
        "---\n",
        "\n",
        "## Required Submission Template\n",
        "\n",
        "**Upload**:\n",
        "\n",
        "the figure (PNG or PDF)\n",
        "\n",
        "OR the notebook cell output\n",
        "\n",
        "Short caption (1–2 sentences)\n",
        "\n",
        "**Title:**  \n",
        "Train vs Validation Loss Curves\n",
        "\n",
        "**What question does this plot answer?**  \n",
        "(1 sentence.)\n",
        "\n",
        "**Description (1–2 sentences):**  \n",
        "What data / model / comparison is shown?  \n",
        "What change did you make, and what happened?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e880ad3",
      "metadata": {
        "id": "7e880ad3"
      },
      "source": [
        "### Starter cell for experiments (students edit this)\n",
        "\n",
        "Edit the hyperparameters below, re-run, and regenerate **Plot 1**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfd017be",
      "metadata": {
        "id": "dfd017be"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# STUDENTS: EDIT THESE\n",
        "# =========================\n",
        "n_train = 64\n",
        "noise_std = 0.25\n",
        "\n",
        "hidden_dim = 256\n",
        "depth = 4\n",
        "dropout = 0.\n",
        "\n",
        "lr = 3e-3\n",
        "weight_decay = 0.\n",
        "batch_size = 32\n",
        "epochs = 300\n",
        "\n",
        "# Recreate data + model, then train\n",
        "x_train, y_train, x_val, y_val = make_sine_data(n_train=n_train, noise_std=noise_std)\n",
        "model = MLPRegressor(hidden_dim=hidden_dim, depth=depth, dropout=dropout).to(device)\n",
        "\n",
        "train_losses, val_losses = train_one_run(\n",
        "    model,\n",
        "    x_train, y_train, x_val, y_val,\n",
        "    lr=lr,\n",
        "    weight_decay=weight_decay,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    print_every=60,\n",
        ")\n",
        "\n",
        "# Plot 1 (required)\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label=\"train loss\")\n",
        "plt.plot(val_losses, label=\"validation loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"MSE loss\")\n",
        "plt.title(\"Train vs Validation Loss Curves (your run)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "best_epoch = int(val_losses.argmin()) + 1\n",
        "print(\"Best validation loss at epoch:\", best_epoch, \"val_loss:\", float(val_losses.min()))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}